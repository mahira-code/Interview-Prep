{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f941e16",
   "metadata": {},
   "source": [
    "# Naive Bayes and Probabilities\n",
    "\n",
    "This notebook demonstrates the fundamentals of **Naive Bayes classification**, highlighting probability computations, assumptions of conditional independence, and example applications.\n",
    "\n",
    "**Author:** Mahira Banu  \n",
    "**Date:** June 2025  \n",
    "**Tags:** #NaiveBayes #ML #Probability #InterviewPrep #GTV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710e7f9-500c-412e-8078-ccceacc5de81",
   "metadata": {},
   "source": [
    "## Joint Probability\n",
    "\n",
    "Joint probability is the chance that **two events happen at the same time**.\n",
    "\n",
    "For example, imagine a dataset where people buy ice cream on days with different weather:\n",
    "\n",
    "| Weather | Buys Ice Cream | Count |\n",
    "|---------|----------------|-------|\n",
    "| Sunny   | Yes            | 30    |\n",
    "| Sunny   | No             | 10    |\n",
    "| Rainy   | Yes            | 5     |\n",
    "| Rainy   | No             | 5     |\n",
    "\n",
    "The total number of observations is 50.\n",
    "\n",
    "The joint probability of it being **Sunny AND buying ice cream** is:\n",
    "\n",
    "$$\n",
    "P(\\text{Sunny} \\cap \\text{Yes}) = \\frac{30}{50} = 0.6 \n",
    "$$\n",
    "This means there is a 60% chance that on a random day, it is sunny and a person buys ice cream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a342dcd-1bdf-4401-83d1-b46087e9018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "Joint Probability P(Sunny ∩ Buys Ice Cream) = 0.60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the dataset\n",
    "data = {\n",
    "    'Weather': ['Sunny', 'Sunny', 'Rainy', 'Rainy'],\n",
    "    'Buys_IceCream': ['Yes', 'No', 'Yes', 'No'],\n",
    "    'Count': [30, 10, 5, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Total observations\n",
    "total = df['Count'].sum()\n",
    "\n",
    "# Calculate joint probability P(Sunny ∩ Yes)\n",
    "joint_count = df[(df['Weather'] == 'Sunny') & (df['Buys_IceCream'] == 'Yes')]['Count'].values[0] \n",
    "\n",
    "#This above line finds how many times the condition \"Weather = Sunny AND Buys Ice Cream = Yes\" occurs in the dataset.\n",
    "\n",
    "joint_prob = joint_count / total\n",
    "\n",
    "print(f\"Joint Probability P(Sunny ∩ Buys Ice Cream) = {joint_prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565c2f5-cdff-453f-9f57-23e9926cb5c4",
   "metadata": {},
   "source": [
    "## Marginal Probability (Ice Cream Example)\n",
    "\n",
    "Marginal probability is the chance of **a single event happening**, regardless of other events.\n",
    "\n",
    "From our ice cream data:\n",
    "\n",
    "| Weather | Buys Ice Cream | Count |\n",
    "|---------|----------------|-------|\n",
    "| Sunny   | Yes            | 30    |\n",
    "| Sunny   | No             | 10    |\n",
    "| Rainy   | Yes            | 5     |\n",
    "| Rainy   | No             | 5     |\n",
    "\n",
    "Total observations: 50\n",
    "\n",
    "To find the probability that a person **buys ice cream regardless of weather**, sum all counts where Buys Ice Cream = Yes:\n",
    "$$\n",
    "P(\\text{Buys Ice Cream = Yes}) = \\frac{30 + 5}{50} = \\frac{35}{50} = 0.7\n",
    "$$\n",
    "\n",
    "So, there is a 70% chance that a randomly observed person buys ice cream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01dcd631-7474-4fc7-aa72-08bf2ace6e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Marginal Probability P(Buys Ice Cream = Yes) = 0.70\n"
     ]
    }
   ],
   "source": [
    "# Calculate marginal probability of Buys Ice Cream = Yes\n",
    "marginal_count_icecream = df[df['Buys_IceCream'] == 'Yes']['Count'].sum()\n",
    "print(df[df['Buys_IceCream'] == 'Yes']['Count'].sum())\n",
    "marginal_prob_icecream = marginal_count_icecream / total\n",
    "\n",
    "print(f\"Marginal Probability P(Buys Ice Cream = Yes) = {marginal_prob_icecream:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d938b6e-13d4-4a0d-8c54-f21a920b75ea",
   "metadata": {},
   "source": [
    "## Conditional Probability (Ice Cream Example)\n",
    "\n",
    "Conditional probability is the chance of an event happening **given that** another event has already happened.\n",
    "\n",
    "From our ice cream data:\n",
    "\n",
    "| Weather | Buys Ice Cream | Count |\n",
    "|---------|----------------|-------|\n",
    "| Sunny   | Yes            | 30    |\n",
    "| Sunny   | No             | 10    |\n",
    "| Rainy   | Yes            | 5     |\n",
    "| Rainy   | No             | 5     |\n",
    "\n",
    "Total observations: 50\n",
    "\n",
    "Suppose we want to find the probability that a person **buys ice cream given that it is sunny**. This is written as:\n",
    "\n",
    "$$\n",
    "P(\\text{Buys Ice Cream = Yes} \\mid \\text{Weather = Sunny}) = \\frac{P(\\text{Buys Ice Cream = Yes} \\cap \\text{Weather = Sunny})}{P(\\text{Weather = Sunny})}\n",
    "$$\n",
    "\n",
    "We already know:\n",
    "$\n",
    "(P(\\text{Buys Ice Cream = Yes} \\cap \\text{Weather = Sunny}) = \\frac{30}{50} = 0.6)\n",
    "$\n",
    "$\n",
    "(P(\\text{Weather = Sunny}) = \\frac{30 + 10}{50} = \\frac{40}{50} = 0.8)\n",
    "$\n",
    "So,\n",
    "$\n",
    "P(\\text{Buys Ice Cream = Yes} \\mid \\text{Weather = Sunny}) = \\frac{0.6}{0.8} = 0.75\n",
    "$\n",
    "\n",
    "This means that given the weather is sunny, there is a 75% chance that a person buys ice cream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a197b320-36e3-498d-b3e4-77c8ae00dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probability P(Buys Ice Cream = Yes | Weather = Sunny) = 0.75\n"
     ]
    }
   ],
   "source": [
    "# Calculate conditional probability P(Buys Ice Cream = Yes | Weather = Sunny)\n",
    "joint_prob = df[(df['Weather'] == 'Sunny') & (df['Buys_IceCream'] == 'Yes')]['Count'].values[0] / total\n",
    "marginal_prob_weather = df[df['Weather'] == 'Sunny']['Count'].sum() / total\n",
    "conditional_prob = joint_prob / marginal_prob_weather\n",
    "\n",
    "print(f\"Conditional Probability P(Buys Ice Cream = Yes | Weather = Sunny) = {conditional_prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ac12f-adfc-438f-80f5-7ccdb4521d0b",
   "metadata": {},
   "source": [
    "## Independence of Events\n",
    "\n",
    "Two events \\(A\\) and \\(B\\) are **independent** if knowing one event does **not affect** the probability of the other.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "P(A \\cap B) = P(A) \\times P(B)\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "$\n",
    "P(A \\cap B)\n",
    "$\n",
    "\n",
    "is the joint probability of both events occurring together, and\n",
    "\n",
    "$\n",
    "P(A) \\text{ and } P(B)\n",
    "$\n",
    "\n",
    "are the marginal probabilities of the individual events.\n",
    "\n",
    "\n",
    "### Ice Cream Example\n",
    "\n",
    "Given the data:\n",
    "\n",
    "| Weather | Buys Ice Cream | Count |\n",
    "|---------|----------------|-------|\n",
    "| Sunny   | Yes            | 30    |\n",
    "| Sunny   | No             | 10    |\n",
    "| Rainy   | Yes            | 5     |\n",
    "| Rainy   | No             | 5     |\n",
    "\n",
    "Calculate:\n",
    "\n",
    "- $P(\\text{Sunny}) = \\frac{40}{50} = 0.8$\n",
    "- $P(\\text{Buys Ice Cream}) = \\frac{35}{50} = 0.7$\n",
    "- $P(\\text{Sunny} \\cap \\text{Buys Ice Cream}) = \\frac{30}{50} = 0.6$\n",
    "\n",
    "\n",
    "Check independence:\n",
    "\n",
    "$$\n",
    "P(\\text{Sunny} \\cap \\text{Buys Ice Cream}) \\stackrel{?}{=} P(\\text{Sunny}) \\times P(\\text{Buys Ice Cream}) = 0.8 \\times 0.7 = 0.56\n",
    "$$\n",
    "\n",
    "Since $0.6 \\neq 0.56$, the events are **not independent**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fbf7e7c-b0c8-4858-a215-7074f5a584e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Sunny) = 0.80\n",
      "P(Buys Ice Cream) = 0.70\n",
      "P(Sunny and Buys Ice Cream) = 0.60\n",
      "Product P(Sunny) * P(Buys Ice Cream) = 0.56\n",
      "Events are NOT independent\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities\n",
    "p_sunny = df[df['Weather'] == 'Sunny']['Count'].sum() / total\n",
    "p_icecream = df[df['Buys_IceCream'] == 'Yes']['Count'].sum() / total\n",
    "p_joint = df[(df['Weather'] == 'Sunny') & (df['Buys_IceCream'] == 'Yes')]['Count'].values[0] / total\n",
    "\n",
    "print(f\"P(Sunny) = {p_sunny:.2f}\")\n",
    "print(f\"P(Buys Ice Cream) = {p_icecream:.2f}\")\n",
    "print(f\"P(Sunny and Buys Ice Cream) = {p_joint:.2f}\")\n",
    "print(f\"Product P(Sunny) * P(Buys Ice Cream) = {p_sunny * p_icecream:.2f}\")\n",
    "\n",
    "if abs(p_joint - (p_sunny * p_icecream)) < 1e-6:\n",
    "    print(\"Events are independent\")\n",
    "else:\n",
    "    print(\"Events are NOT independent\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2b1d01-5e26-4d6b-b198-e2df1df2d564",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem relates the conditional and marginal probabilities of random events. It is given by:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P(Y \\mid X)$ is the posterior probability: the probability of event $Y$ given $X$.\n",
    "- $P(X \\mid Y)$ is the likelihood: the probability of event $X$ given $Y$.\n",
    "- $P(Y)$ is the prior probability of event $Y$.\n",
    "- $P(X)$ is the marginal likelihood or evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46c25144-a530-4749-80ae-da57fb904e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam given the word: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Bayes Theorem example: Spam detection\n",
    "\n",
    "# Probabilities (assumed values for example)\n",
    "P_spam = 0.2               # Prior probability of spam\n",
    "P_word_given_spam = 0.7    # Probability word appears in spam\n",
    "P_word_given_not_spam = 0.1 # Probability word appears in non-spam\n",
    "P_not_spam = 1 - P_spam\n",
    "\n",
    "# Apply Bayes Theorem to calculate P(spam | word)\n",
    "# Posterior = (Likelihood * Prior) / Evidence\n",
    "\n",
    "P_word = P_word_given_spam * P_spam + P_word_given_not_spam * P_not_spam  # Total probability of word\n",
    "\n",
    "P_spam_given_word = (P_word_given_spam * P_spam) / P_word\n",
    "\n",
    "print(f\"Probability of spam given the word: {P_spam_given_word:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1596b-2566-434f-b4b7-5ddb2d232311",
   "metadata": {},
   "source": [
    "## Naïve Bayes Assumption\n",
    "\n",
    "**Definition:**  \n",
    "All features are conditionally independent given the class.\n",
    "\n",
    "This means that, for a given class \\( C \\), the probability of observing features \\( X_1, X_2, ..., X_n \\) is the product of the probabilities of observing each feature individually, assuming independence:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, ..., X_n \\mid C) = P(X_1 \\mid C) \\times P(X_2 \\mid C) \\times \\cdots \\times P(X_n \\mid C)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Example: Spam Filtering\n",
    "\n",
    "If we want to calculate the probability of an email \\( E \\) being spam \\( S \\), and the email has words \\( w_1, w_2, ..., w_n \\), Naïve Bayes assumes:\n",
    "\n",
    "$$\n",
    "P(E \\mid S) \\approx P(w_1 \\mid S) \\times P(w_2 \\mid S) \\times \\cdots \\times P(w_n \\mid S)\n",
    "$$\n",
    "\n",
    "In simple words:  \n",
    "The probability of the entire email given it’s spam is approximated by multiplying the probabilities of each word occurring given that it is spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "082f9988-5c8c-4461-80a5-1af4de29c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: 'Free prize for you' -> Prediction: Spam\n",
      "Email: 'Let's schedule a meeting' -> Prediction: Not Spam\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample emails\n",
    "emails = [\n",
    "    \"Free money offer just for you\",\n",
    "    \"Hi Bob, are we meeting tomorrow?\",\n",
    "    \"Congratulations, you won a prize\",\n",
    "    \"Dear friend, let's catch up soon\",\n",
    "    \"Win a free vacation now\",\n",
    "    \"Are you available for a call?\"\n",
    "]\n",
    "\n",
    "# Labels: 1 for spam, 0 for not spam\n",
    "labels = [1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Convert text data to numeric features (word counts)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X, labels)\n",
    "\n",
    "# Predict if new email is spam\n",
    "new_emails = [\"Free prize for you\", \"Let's schedule a meeting\"]\n",
    "X_new = vectorizer.transform(new_emails)\n",
    "predictions = nb.predict(X_new)\n",
    "\n",
    "for email, pred in zip(new_emails, predictions):\n",
    "    label = \"Spam\" if pred == 1 else \"Not Spam\"\n",
    "    print(f\"Email: '{email}' -> Prediction: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92f695-1c5d-4e79-9e7e-c3199fdd0956",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "**Definition:**  \n",
    "Laplace smoothing is a technique used to handle the problem of zero probabilities in probabilistic models like Naïve Bayes.\n",
    "\n",
    "When a feature (e.g., a word) does **not appear** in the training data for a certain class, the probability estimate for that feature would be zero, which can cause the whole probability product to become zero.\n",
    "\n",
    "Laplace smoothing adds a small value (usually 1) to all counts to avoid zero probabilities.\n",
    "\n",
    "\n",
    "### Formula:\n",
    "\n",
    "If \\( \\text{count}(X_i, C) \\) is the count of feature \\( X_i \\) in class \\( C \\), and \\( V \\) is the total number of possible features (vocabulary size), then the smoothed probability is:\n",
    "\n",
    "$$\n",
    "P(X_i \\mid C) = \\frac{\\text{count}(X_i, C) + 1}{\\text{count}(C) + V}\n",
    "$$\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "If the word \"free\" never appears in the \"not spam\" class in the training data, without smoothing:\n",
    "\n",
    "$$\n",
    "P(\\text{\"free\"} \\mid \\text{not spam}) = 0\n",
    "$$\n",
    "\n",
    "With Laplace smoothing:\n",
    "\n",
    "$$\n",
    "P(\\text{\"free\"} \\mid \\text{not spam}) = \\frac{0 + 1}{\\text{total words in not spam} + V}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d0c53fe1-c36e-4f93-afb9-ac26e89669b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of not spam: 0.6338\n",
      "Probability of spam: 0.3662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample emails\n",
    "emails = [\n",
    "    \"Free money offer just for you\",\n",
    "    \"Hi Bob, are we meeting tomorrow?\",\n",
    "    \"Congratulations, you won a prize\",\n",
    "    \"Dear friend, let's catch up soon\",\n",
    "    \"Win a free vacation now\",\n",
    "    \"Are you available for a call?\"\n",
    "]\n",
    "\n",
    "# Labels: 1 for spam, 0 for not spam\n",
    "labels = [1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Convert text data to numeric features (word counts)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train Naive Bayes classifier with Laplace smoothing (alpha=1)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X, labels)\n",
    "\n",
    "# Predict probabilities for a new email containing unseen word \"winner\"\n",
    "new_emails = [\"You are a winner!\"]\n",
    "X_new = vectorizer.transform(new_emails)\n",
    "probs = nb.predict_proba(X_new)\n",
    "\n",
    "print(f\"Probability of not spam: {probs[0][0]:.4f}\")\n",
    "print(f\"Probability of spam: {probs[0][1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e1aa7-3cf2-4062-898f-65cc6f3b0f92",
   "metadata": {},
   "source": [
    "## Log Probabilities\n",
    "\n",
    "**Definition:**  \n",
    "When multiplying many small probabilities (like in Naïve Bayes), the result can become extremely small, causing numerical underflow (computers round to zero).\n",
    "\n",
    "To avoid this, we use **logarithms** to convert multiplication into addition, which is numerically more stable.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "Instead of computing:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X) = P(Y) \\times \\prod_{i=1}^n P(X_i \\mid Y)\n",
    "$$\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "\\log P(Y \\mid X) = \\log P(Y) + \\sum_{i=1}^n \\log P(X_i \\mid Y)\n",
    "$$\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- Multiplying tiny probabilities → risk of underflow  \n",
    "- Adding log probabilities → stable and efficient  \n",
    "- The class with the highest log posterior is predicted.\n",
    "## Log Probabilities\n",
    "\n",
    "**Definition:**  \n",
    "When multiplying many small probabilities (like in Naïve Bayes), the result can become extremely small, causing numerical underflow (computers round to zero).\n",
    "\n",
    "To avoid this, we use **logarithms** to convert multiplication into addition, which is numerically more stable.\n",
    "\n",
    "\n",
    "### Formula:\n",
    "\n",
    "Instead of computing:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X) = P(Y) \\times \\prod_{i=1}^n P(X_i \\mid Y)\n",
    "$$\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "\\log P(Y \\mid X) = \\log P(Y) + \\sum_{i=1}^n \\log P(X_i \\mid Y)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- Multiplying tiny probabilities → risk of underflow  \n",
    "- Adding log probabilities → stable and efficient  \n",
    "- The class with the highest log posterior is predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "def450ee-0937-4e34-9c81-b276ba77570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log P('are'|spam) = -3.6889\n",
      "Log P('available'|spam) = -3.6889\n",
      "Log P('bob'|spam) = -3.6889\n",
      "Log P('call'|spam) = -3.6889\n",
      "Log P('catch'|spam) = -3.6889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample emails\n",
    "emails = [\n",
    "    \"Free money offer just for you\",\n",
    "    \"Hi Bob, are we meeting tomorrow?\",\n",
    "    \"Congratulations, you won a prize\",\n",
    "    \"Dear friend, let's catch up soon\",\n",
    "    \"Win a free vacation now\",\n",
    "    \"Are you available for a call?\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X, labels)\n",
    "\n",
    "# Access log probabilities for class spam (1)\n",
    "log_probs_spam = nb.feature_log_prob_[1]\n",
    "\n",
    "# Show log probability for first 5 features\n",
    "for word, log_prob in zip(vectorizer.get_feature_names_out()[:5], log_probs_spam[:5]):\n",
    "    print(f\"Log P('{word}'|spam) = {log_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80619b98-dd80-40dc-ac7c-b9454e7b1cda",
   "metadata": {},
   "source": [
    "## Zero-One Loss Function\n",
    "\n",
    "**Definition:**  \n",
    "The Zero-One Loss function is a simple way to measure prediction errors. It assigns:\n",
    "\n",
    "- A loss of 0 if the prediction is **correct**.\n",
    "- A loss of 1 if the prediction is **incorrect**.\n",
    "\n",
    "\n",
    "### Formula:\n",
    "\n",
    "For true label \\( y \\) and predicted label \\( \\hat{y} \\):\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \n",
    "\\begin{cases}\n",
    "0, & \\text{if } y = \\hat{y} \\\\\n",
    "1, & \\text{if } y \\neq \\hat{y}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- This loss treats all errors equally.\n",
    "- It's used mostly in classification tasks to count misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "def51199-e62f-48a9-8564-236a9cd84405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Zero-One Loss: 2\n"
     ]
    }
   ],
   "source": [
    "def zero_one_loss(y_true, y_pred):\n",
    "    losses = [0 if true == pred else 1 for true, pred in zip(y_true, y_pred)]\n",
    "    return sum(losses)\n",
    "\n",
    "# Example usage:\n",
    "true_labels = [1, 0, 1, 1, 0]\n",
    "predicted_labels = [1, 0, 0, 1, 1]\n",
    "\n",
    "loss = zero_one_loss(true_labels, predicted_labels)\n",
    "print(f\"Total Zero-One Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2cc48-8036-444a-a941-ef09f9c5f2bc",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "**Definition:**  \n",
    "A probabilistic classifier based on Bayes' Theorem with the \"naïve\" assumption of conditional independence between features.\n",
    "\n",
    "\n",
    "\n",
    "### How it works:\n",
    "\n",
    "Given a feature vector \\( X = (X_1, X_2, ..., X_n) \\) and class \\( Y \\), the classifier computes the posterior probability for each class:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X) = \\frac{P(Y) \\times \\prod_{i=1}^{n} P(X_i \\mid Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Since \\( P(X) \\) is the same for all classes, we predict the class that maximizes the numerator:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\arg\\max_Y \\left[ P(Y) \\times \\prod_{i=1}^n P(X_i \\mid Y) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- Calculate prior \\( P(Y) \\) from class frequencies.\n",
    "- Calculate likelihood \\( P(X_i \\mid Y) \\) for each feature given class.\n",
    "- Multiply priors and likelihoods to get the posterior (up to proportionality).\n",
    "- Predict the class with the highest posterior probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "721ea79b-459b-48df-83ab-27778ab87205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample text data\n",
    "emails = [\n",
    "    \"Win money now\",\n",
    "    \"Hello friend, how are you?\",\n",
    "    \"Claim your free prize\",\n",
    "    \"Are we still meeting tomorrow?\",\n",
    "    \"Exclusive offer just for you\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]  # 1=spam, 0=not spam\n",
    "\n",
    "# Convert text to numeric features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab1b23-1e55-46f7-8ecc-44b68bd39770",
   "metadata": {},
   "source": [
    "## Graphical Model of Naïve Bayes\n",
    "\n",
    "Naïve Bayes can be represented as a **probabilistic graphical model** where:\n",
    "\n",
    "- The class variable \\( Y \\) is the **parent node**.\n",
    "- Each feature \\( X_i \\) is a **child node** dependent only on \\( Y \\).\n",
    "\n",
    "This expresses the **conditional independence assumption**:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, ..., X_n \\mid Y) = \\prod_{i=1}^n P(X_i \\mid Y)\n",
    "$$\n",
    "\n",
    "### Diagram (Simple):\n",
    "\n",
    " Y\n",
    " \n",
    " /   |   \\\n",
    "\n",
    " x1  x2   x3\n",
    "\n",
    " \n",
    "\n",
    "This means each feature is conditionally independent of others given the class.\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "Knowing the class, features are independent — so we only need to learn the distribution of each feature given the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3a8af87d-d389-4f7e-b95f-571822063bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Naive Bayes models implement this assumption internally\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Features X1, X2, ..., Xn are assumed independent given class Y\n",
    "# The model learns P(X_i | Y) for each feature separately and P(Y) as class prior\n",
    "nb = MultinomialNB()\n",
    "# When you fit nb.fit(X_train, y_train), the model learns P(X_i|Y) and P(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119c47b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook covered the basics of Naive Bayes — its use of prior and likelihood to estimate posterior probabilities. These concepts are especially useful in NLP, spam filtering, and real-world probabilistic models.\n",
    "\n",
    "This notebook contributes to my public portfolio for Kaggle, GitHub, and GTV preparation.\n",
    "Thank you Keep Learning👩🏼‍💻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
